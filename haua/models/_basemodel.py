from typing import Union, Optional, Type, Callable, Any, Dict, List

import torch
import torch.nn as nn

from ..utils import get_script_name
from .utils import FreezeMixin


_SCRIPT_NAME = get_script_name(__file__)


BASE_MODEL_FREEZE_PRESETS = {
    # åŸºç¡€é¢„è®¾
    'none': [],
    'all': ['backbone', 'neck', 'head'],
    
    # å•æ¨¡å—å†»ç»“
    'backbone_only': ['backbone'],
    'neck_only': ['neck'],
    'head_only': ['head'],
    
    # ç»„åˆå†»ç»“
    'freeze_backbone_neck': ['backbone', 'neck'],
    'freeze_backbone_head': ['backbone', 'head'],
    'freeze_neck_head': ['neck', 'head'],
    
    # å¸¸ç”¨è®­ç»ƒç­–ç•¥
    'train_head_only': ['backbone', 'neck'],           # åªè®­ç»ƒhead
    'train_neck_head': ['backbone'],                    # è®­ç»ƒneckå’Œhead
    'finetune_all_from_backbone': [],                   # å…¨éƒ¨è®­ç»ƒï¼ˆå¾®è°ƒï¼‰
    
    # æ¸è¿›å¼è§£å†»é˜¶æ®µ
    'stage1_head_warmup': ['backbone', 'neck'],
    'stage2_neck_tuning': ['backbone'],
    'stage3_full_tuning': []}


class BaseModel(nn.Module, FreezeMixin):
    """
    é€šç”¨æ¨¡å‹åŸºç±»ï¼Œæ”¯æŒä»¥ä¸‹åŠŸèƒ½ï¼š
        1. çµæ´»çš„æ¨¡å—å®šä¹‰ï¼ˆå®ä¾‹/ç±»ï¼‰
        2. è‡ªåŠ¨å‚æ•°å†»ç»“ï¼ˆç»§æ‰¿è‡ª FreezeMixinï¼‰
        3. æ¨¡å‹èåˆï¼ˆfuseï¼‰
        4. è‡ªå®šä¹‰åå¤„ç†
    
    ç‰¹æ€§ï¼š
        - æ‰€æœ‰ç»§æ‰¿æ­¤ç±»çš„æ¨¡å‹è‡ªåŠ¨è·å¾—å‚æ•°å†»ç»“èƒ½åŠ›
        - æä¾›é’ˆå¯¹ backbone/neck/head ç»“æ„çš„ä¾¿æ·å†»ç»“æ–¹æ³•
        - æ”¯æŒæ¸è¿›å¼è®­ç»ƒçš„é¢„è®¾é…ç½®
    """

    def __init__(
        self,
        backbone: Union[nn.Module, Type[nn.Module]],
        neck: Optional[Union[nn.Module, Type[nn.Module]]] = None,
        head: Optional[Union[nn.Module, Type[nn.Module]]] = None,
        custom_postprocess: Optional[Callable[[Any], Any]] = None,
        module_configs: Optional[Dict[str, dict]] = None,
        freeze_patterns: Optional[List[str]] = None,
        freeze_preset: Optional[str] = None,
        verbose_freeze: bool = True,
    ):
        """
        Args:
            backbone: Backbone æ¨¡å—ï¼ˆå®ä¾‹æˆ–ç±»ï¼‰
            neck: Neck æ¨¡å—ï¼ˆå®ä¾‹æˆ–ç±»ï¼‰
            head: Head æ¨¡å—ï¼ˆå®ä¾‹æˆ–ç±»ï¼‰
            custom_postprocess: è‡ªå®šä¹‰åå¤„ç†å‡½æ•°
            module_configs: æ¨¡å—é…ç½®å­—å…¸
            freeze_patterns: å†»ç»“æ¨¡å¼åˆ—è¡¨ï¼Œä¾‹å¦‚ ['backbone', 'neck']
            freeze_preset: ä½¿ç”¨é¢„è®¾å†»ç»“é…ç½®ï¼Œä¾‹å¦‚ 'train_head_only'
            verbose_freeze: æ˜¯å¦æ‰“å°å†»ç»“è¯¦æƒ…
        """
        super().__init__()
        # åˆå§‹åŒ–æ¨¡å—é…ç½®
        self.module_configs = module_configs or {}
        # åˆå§‹åŒ–ä¸‰å¤§æ¨¡å—
        self.backbone = self._init_module("backbone", backbone)
        self.neck = self._init_module("neck", neck) if neck else nn.Identity()
        self.head = self._init_module("head", head) if head else nn.Identity()
        # åå¤„ç†å‡½æ•°
        self.custom_postprocess = custom_postprocess
        # åˆå§‹åŒ– Freezerï¼ˆæ¥è‡ª FreezeMixinï¼‰
        self.setup_freezer(verbose=verbose_freeze)
        # åº”ç”¨å†»ç»“ç­–ç•¥ï¼ˆé¢„è®¾ä¼˜å…ˆï¼‰
        if freeze_preset:
            self.freeze_by_preset(freeze_preset)
        elif freeze_patterns:
            self.freeze_parameters(freeze_patterns)

    def _init_module(
        self,
        name: str,
        module_or_class: Union[nn.Module, Type[nn.Module]]
    ) -> nn.Module:
        """åˆå§‹åŒ–æ¨¡å—ï¼šå®ä¾‹ç›´æ¥è¿”å›ï¼›ç±»åˆ™ç”¨ config å®ä¾‹åŒ–"""
        if isinstance(module_or_class, nn.Module):
            return module_or_class
        elif isinstance(module_or_class, type) and issubclass(module_or_class, nn.Module):
            cfg = self.module_configs.get(name, {})
            return module_or_class(**cfg)
        else:
            raise TypeError(
                f"{name} å¿…é¡»æ˜¯ nn.Module å®ä¾‹ æˆ– nn.Module å­ç±»ï¼Œ"
                f"å½“å‰ç±»å‹ä¸º {type(module_or_class)}")
    
    def forward_backbone(self, x: torch.Tensor) -> Any:
        """Backboneå‰å‘ä¼ æ’­"""
        return self.backbone(x)

    def forward_neck(self, feats: Any) -> Any:
        """Neckå‰å‘ä¼ æ’­"""
        return self.neck(feats)

    def forward_head(self, feats: Any) -> Any:
        """Headå‰å‘ä¼ æ’­"""
        return self.head(feats)

    def forward(self, x: torch.Tensor, *args, **kwargs) -> Any:
        """å®Œæ•´å‰å‘ä¼ æ’­"""
        feats = self.forward_backbone(x)
        fused = self.forward_neck(feats)
        out = self.forward_head(fused)
        
        if self.custom_postprocess is not None:
            out = self.custom_postprocess(out)

        return feats, fused, out
    
    def freeze_by_preset(self, preset_name: str):
        """
        ä½¿ç”¨é¢„è®¾é…ç½®å†»ç»“å‚æ•°
        
        Args:
            preset_name: é¢„è®¾åç§°ï¼Œå¯é€‰ï¼š
                - 'none': ä¸å†»ç»“
                - 'backbone_only': åªå†»ç»“backbone
                - 'train_head_only': åªè®­ç»ƒhead
                - 'stage1_head_warmup': æ¸è¿›å¼è®­ç»ƒé˜¶æ®µ1
                ç­‰ç­‰ï¼Œè¯¦è§ BASE_MODEL_FREEZE_PRESETS
        """
        if preset_name not in BASE_MODEL_FREEZE_PRESETS:
            raise ValueError(
                f"æœªçŸ¥é¢„è®¾: {preset_name}\n"
                f"å¯ç”¨é¢„è®¾: {list(BASE_MODEL_FREEZE_PRESETS.keys())}")
        
        patterns = BASE_MODEL_FREEZE_PRESETS[preset_name]
        
        if self._freezer.verbose:
            print(f"\nğŸ¯ åº”ç”¨å†»ç»“é¢„è®¾: '{preset_name}'")
        
        return self.freeze_parameters(patterns)
    
    def freeze_backbone(self):
        """ä¾¿æ·æ–¹æ³•ï¼šå†»ç»“Backbone"""
        return self.freeze_parameters(['backbone'])
    
    def freeze_neck(self):
        """ä¾¿æ·æ–¹æ³•ï¼šå†»ç»“Neck"""
        return self.freeze_parameters(['neck'])
    
    def freeze_head(self):
        """ä¾¿æ·æ–¹æ³•ï¼šå†»ç»“Head"""
        return self.freeze_parameters(['head'])
    
    def unfreeze_backbone(self):
        """ä¾¿æ·æ–¹æ³•ï¼šè§£å†»Backbone"""
        return self.unfreeze_parameters(['backbone'])
    
    def unfreeze_neck(self):
        """ä¾¿æ·æ–¹æ³•ï¼šè§£å†»Neck"""
        return self.unfreeze_parameters(['neck'])
    
    def unfreeze_head(self):
        """ä¾¿æ·æ–¹æ³•ï¼šè§£å†»Head"""
        return self.unfreeze_parameters(['head'])
    
    def get_standard_parameter_groups(
        self,
        lr_backbone: float = 1e-5,
        lr_neck: float = 1e-4,
        lr_head: float = 1e-3,
        weight_decay: float = 5e-4
    ) -> List[Dict]:
        """
        è·å–æ ‡å‡†çš„å‚æ•°åˆ†ç»„ï¼ˆç”¨äºä¼˜åŒ–å™¨ï¼‰
        
        Args:
            lr_backbone: Backboneå­¦ä¹ ç‡
            lr_neck: Neckå­¦ä¹ ç‡
            lr_head: Headå­¦ä¹ ç‡
            weight_decay: æƒé‡è¡°å‡
        
        Returns:
            å‚æ•°åˆ†ç»„åˆ—è¡¨ï¼Œå¯ç›´æ¥ä¼ å…¥ä¼˜åŒ–å™¨
        
        Example:
            >>> param_groups = model.get_standard_parameter_groups()
            >>> optimizer = torch.optim.AdamW(param_groups)
        """
        group_patterns = {
            'backbone': ['backbone'],
            'neck': ['neck'],
            'head': ['head']}
        
        param_dict = self.get_parameter_groups(group_patterns)
        
        param_groups = []
        if 'backbone' in param_dict:
            param_groups.append({
                'params': param_dict['backbone'],
                'lr': lr_backbone,
                'weight_decay': weight_decay})
        if 'neck' in param_dict:
            param_groups.append({
                'params': param_dict['neck'],
                'lr': lr_neck,
                'weight_decay': weight_decay})
        if 'head' in param_dict:
            param_groups.append({
                'params': param_dict['head'],
                'lr': lr_head,
                'weight_decay': weight_decay})
        
        return param_groups
    
    def print_module_summary(self):
        """æ‰“å°æ¨¡å—æ‘˜è¦ï¼ˆåŒ…å«å†»ç»“çŠ¶æ€ï¼‰"""
        print(f"\n{'='*80}")
        print(f"{'æ¨¡å‹æ‘˜è¦':^80}")
        print(f"{'='*80}")
        print(f"æ¨¡å‹ç±»å: {self.class_name}")
        print(f"{'-'*80}")
        
        # ç»Ÿè®¡å„æ¨¡å—å‚æ•°
        module_stats = {}
        for module_name in ['backbone', 'neck', 'head']:
            module = getattr(self, module_name)
            total = sum(p.numel() for p in module.parameters())
            trainable = sum(p.numel() for p in module.parameters() if p.requires_grad)
            frozen = total - trainable
            
            module_stats[module_name] = {
                'class': module.__class__.__name__,
                'total': total,
                'trainable': trainable,
                'frozen': frozen,
                'status': 'ğŸ”“ è®­ç»ƒä¸­' if trainable > 0 else 'ğŸ”’ å·²å†»ç»“'}
        
        # æ‰“å°è¡¨æ ¼
        print(f"{'æ¨¡å—':<15} {'ç±»å':<30} {'æ€»å‚æ•°':>15} {'å¯è®­ç»ƒ':>15} {'çŠ¶æ€':>10}")
        print(f"{'-'*80}")
        
        for name, stats in module_stats.items():
            print(
                f"{name:<15} "
                f"{stats['class']:<30} "
                f"{stats['total']:>15,} "
                f"{stats['trainable']:>15,} "
                f"{stats['status']:>10}")
        
        print(f"{'-'*80}")
        total_all = sum(s['total'] for s in module_stats.values())
        trainable_all = sum(s['trainable'] for s in module_stats.values())
        print(
            f"{'åˆè®¡':<15} "
            f"{'':<30} "
            f"{total_all:>15,} "
            f"{trainable_all:>15,} "
            f"({100*trainable_all/total_all:.1f}%)")
        print(f"{'='*80}")
        
        if self.custom_postprocess:
            print(f"âœ“ è‡ªå®šä¹‰åå¤„ç†: å·²å¯ç”¨")
        print()
    
    def summary(self) -> Dict[str, str]:
        """è¿”å›æ¨¡å‹æ‘˜è¦å­—å…¸"""
        return {
            "backbone": self.backbone.__class__.__name__,
            "neck": self.neck.__class__.__name__,
            "head": self.head.__class__.__name__,
            "has_custom_postprocess": str(self.custom_postprocess is not None)}

    def set_postprocess(self, fn: Callable[[Any], Any]):
        """è®¾ç½®åå¤„ç†å‡½æ•°"""
        self.custom_postprocess = fn

    def fuse(self, verbose: bool = True) -> "BaseModel":
        """è‡ªåŠ¨ fuse æ¨¡å‹ä¸­æ‰€æœ‰æ”¯æŒ .fuse() çš„å­æ¨¡å—ï¼ˆå¦‚ ConvBNActï¼‰"""
        fused_count = 0

        def _fuse_recursive(module: nn.Module) -> int:
            count = 0
            if hasattr(module, 'fuse') and callable(getattr(module, 'fuse')):
                if hasattr(module, 'bn'):  # é’ˆå¯¹ ConvBNAct ç±»
                    module.fuse()
                    count += 1
            for child in module.children():
                count += _fuse_recursive(child)
            return count

        for name in ["backbone", "neck", "head"]:
            module = getattr(self, name, None)
            if module is not None:
                fused_count += _fuse_recursive(module)
        
        if verbose:
            print(f"âœ… BaseModel.fuse(): æˆåŠŸèåˆ {fused_count} ä¸ªå¯èåˆæ¨¡å—ã€‚")

        return self
    
    @property
    def class_name(self):
        """è·å–ç±»å"""
        return self.__class__.__name__